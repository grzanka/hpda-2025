{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f24949",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "\n",
    "## Memory Management and Performance Fundamentals\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "\n",
    "- **Memory Management**: Understanding how NumPy handles memory allocation and why it matters for performance\n",
    "- **Array Operations**: Efficient vs inefficient operations and their impact on computational speed\n",
    "- **Storage Strategies**: Comparing different approaches to data persistence and I/O optimization\n",
    "- **System Resource Monitoring**: Tools and techniques for tracking memory usage in data-intensive applications\n",
    "\n",
    "### Why This Matters for High Performance Data Analysis\n",
    "\n",
    "In high performance computing and data analysis, understanding memory patterns is crucial because:\n",
    "\n",
    "1. **Memory is often the bottleneck** - not CPU speed\n",
    "2. **Memory allocation/deallocation overhead** can dominate computation time\n",
    "3. **Cache efficiency** depends on data layout and access patterns\n",
    "4. **Scalability** requires understanding memory consumption patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac16ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a57a1f",
   "metadata": {},
   "source": [
    "### Check Available Memory\n",
    "\n",
    "Let's start by checking how much RAM is currently available on this system. This is crucial for planning data operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196ea388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available RAM: 2.69 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14e85b",
   "metadata": {},
   "source": [
    "## Large Array Creation and Memory Patterns\n",
    "\n",
    "### Understanding NumPy Memory Efficiency\n",
    "\n",
    "NumPy arrays features:\n",
    "\n",
    "- **Store data in contiguous memory blocks** - enabling efficient CPU cache usage\n",
    "- **Use homogeneous data types** - eliminating Python object overhead\n",
    "- **Enable vectorized operations** - leveraging optimized C/Fortran libraries (BLAS, LAPACK)\n",
    "\n",
    "Let's create a large array to explore memory usage patterns. We'll use `np.arange()` which creates consecutive integers efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5f334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_array = np.arange(20_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde20c21",
   "metadata": {},
   "source": [
    "### Memory Usage Analysis\n",
    "\n",
    "Understanding how much memory our arrays consume is critical for:\n",
    "\n",
    "- **Capacity planning** - ensuring we don't exceed available RAM\n",
    "- **Performance optimization** - predicting cache behavior\n",
    "- **Scaling decisions** - estimating resource needs for larger datasets\n",
    "\n",
    "**Note:** 20 million integers √ó 8 bytes per int64 = ~152 MB theoretical minimum. The actual memory usage may be slightly higher due to Python object overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568341b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by big_array: 152.588 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory used by big_array: {sys.getsizeof(big_array) / (1024 ** 2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e630a82",
   "metadata": {},
   "source": [
    "## Array Growth Patterns and Performance Anti-Patterns\n",
    "\n",
    "### The np.append() Performance Trap\n",
    "\n",
    "One of the most common performance mistakes in NumPy is repeatedly using `np.append()`. Let's explore why this is problematic:\n",
    "\n",
    "**Key Issue:** NumPy arrays have **fixed size** - they cannot grow in-place like Python lists. Every `np.append()` operation:\n",
    "\n",
    "1. **Allocates a completely new array** (size n+1)\n",
    "2. **Copies all existing data** to the new memory location  \n",
    "3. **Adds the new element**\n",
    "4. **Deallocates the old array**\n",
    "\n",
    "This creates **bottleneck** for building arrays element by element!\n",
    "\n",
    "Let's observe this behavior by monitoring memory usage as we append elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5409a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by big_array after appending one element: 152.588 MB\n"
     ]
    }
   ],
   "source": [
    "# add one integer at the end of the list, check its size and memory usage\n",
    "big_array = np.append(big_array, 1)\n",
    "print(f\"Memory used by big_array after appending one element: {sys.getsizeof(big_array) / (1024 ** 2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830134a9",
   "metadata": {},
   "source": [
    "### Single Append Operation\n",
    "\n",
    "Watch what happens when we add just **one** element to our large array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879176b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by big_array after appending one element: 152.588 MB\n"
     ]
    }
   ],
   "source": [
    "# add one integer at the end of the list, check its size and memory usage\n",
    "big_array = np.append(big_array, 1)\n",
    "print(f\"Memory used by big_array after appending one element: {sys.getsizeof(big_array) / (1024 ** 2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70ea71",
   "metadata": {},
   "source": [
    "### Repeated Append Operations\n",
    "\n",
    "Notice how each append operation requires copying the **entire array**. Let's repeat this and see the cumulative effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e40c1",
   "metadata": {},
   "source": [
    "### Performance Impact of Multiple Appends\n",
    "\n",
    "Now let's append 100 elements and observe the **catastrophic** memory and performance impact:\n",
    "\n",
    "**What's happening:** Each append operation:\n",
    "- Creates a new array with 20,000,000+ elements\n",
    "- Copies ~152 MB of data \n",
    "- Does this 100 times = ~15.2 **GB** of unnecessary memory operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fb33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try appending 100 times\n",
    "for i in range(100):\n",
    "    size_of_big_array = sys.getsizeof(big_array)\n",
    "    big_array = np.append(big_array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4a154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by big_array after appending one element: 152.589 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory used by big_array after appending one element: {sys.getsizeof(big_array) / (1024 ** 2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79300c6f",
   "metadata": {},
   "source": [
    "## Data Persistence and I/O Optimization\n",
    "\n",
    "### Efficient Data Storage Strategies\n",
    "\n",
    "In high performance data analysis, I/O operations often become bottlenecks. NumPy provides several formats for storing arrays efficiently:\n",
    "\n",
    "- **`.npz` format**: NumPy's native binary format\n",
    "- **Uncompressed**: Fast write/read, larger file size  \n",
    "- **Compressed**: Slower write/read, smaller file size\n",
    "\n",
    "The choice depends on your workflow:\n",
    "- **Temporary data**: Use uncompressed for speed\n",
    "- **Long-term storage**: Use compression to save disk space\n",
    "- **Network transfer**: Compression reduces bandwidth usage\n",
    "\n",
    "Let's compare both approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9899393",
   "metadata": {},
   "source": [
    "### Uncompressed Storage with np.savez()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895c31f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file size on disk: 152.589 MB\n",
      "Memory used by big_array: 152.589 MB\n"
     ]
    }
   ],
   "source": [
    "output_file = 'big_array.npz'\n",
    "np.savez(output_file, big_array=big_array)\n",
    "\n",
    "# output file size on disk in MB, memory footprint of numpy array in MB\n",
    "print(f\"Output file size on disk: {os.path.getsize(output_file) / (1024 ** 2):.3f} MB\")\n",
    "print(f\"Memory used by big_array: {sys.getsizeof(big_array) / (1024 ** 2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891d9c2",
   "metadata": {},
   "source": [
    "### Compressed Storage with np.savez_compressed()\n",
    "\n",
    "Now let's try the same data with compression enabled:\n",
    "\n",
    "**Key Insight:** Our array contains sequential integers (0, 1, 2, 3...), which compress **extremely well** because of the predictable pattern. Real-world data may not compress as efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc5f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file size on disk (compressed): 28.873 MB\n",
      "Memory used by big_array: 152.589 MB\n"
     ]
    }
   ],
   "source": [
    "output_file_compressed = 'big_array_compressed.npz'\n",
    "np.savez_compressed(output_file_compressed, big_array=big_array)\n",
    "# output file size on disk in MB, memory footprint of numpy array in MB\n",
    "print(f\"Output file size on disk (compressed): {os.path.getsize(output_file_compressed) / (1024 ** 2):.3f} MB\")\n",
    "print(f\"Memory used by big_array: {sys.getsizeof(big_array) / (1024 ** 2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22de6d",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways and Exercises\n",
    "\n",
    "### Summary: Critical Performance Lessons\n",
    "\n",
    "1. **Memory Management**: Always monitor your memory usage in data-intensive applications\n",
    "2. **Array Growth**: Never use `np.append()` in loops - pre-allocate arrays when possible\n",
    "3. **I/O Strategy**: Choose appropriate compression based on your use case\n",
    "4. **Scalability**: Understanding these patterns is essential for working with larger datasets\n",
    "\n",
    "### üîç **Analysis Question**\n",
    "**Why is repeatedly calling `np.append(big_array, 1)` so inefficient?**\n",
    "\n",
    "*Think about: memory allocation, data copying, and time complexity*\n",
    "\n",
    "### üèãÔ∏è **Performance Exercise** \n",
    "**Challenge:** Increase the size of the array until it reaches 10 GB of memory usage. \n",
    "\n",
    "Requirements:\n",
    "- Print memory usage each time the array grows\n",
    "- Monitor your system's available RAM \n",
    "- Stop before running out of memory!\n",
    "\n",
    "**Hints:**\n",
    "- Use `np.arange()` or `np.zeros()` to create large arrays efficiently\n",
    "- Calculate target array size: 10 GB √∑ 8 bytes per int64 = ~1.25 billion elements\n",
    "- Consider creating arrays in chunks if needed\n",
    "\n",
    "### üßÆ **Advanced Challenge**\n",
    "Compare the performance of these array creation methods for building large arrays:\n",
    "1. Using `np.append()` in a loop (small arrays only!)\n",
    "2. Pre-allocating with `np.zeros()` then filling values\n",
    "3. Using `np.arange()` directly\n",
    "4. Using `np.concatenate()` with smaller pre-built chunks\n",
    "\n",
    "Time each method and explain the performance differences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9e762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
